{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split by date and Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "import dask.bag as db\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/frankdai/data/trade-20200607-20200610/trade'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLATTEN_DIR = '/home/frankdai/data/trade-20200607-20200610'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchanges = os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exchange in os.listdir(DATA_DIR):\n",
    "    [ex,market] = exchange.split('-')\n",
    "    pairs = os.listdir(os.path.join(DATA_DIR, exchange))\n",
    "    for pair in pairs:\n",
    "        files = os.listdir(os.path.join(DATA_DIR, exchange, pair))\n",
    "        for file in [x for x in files if x.endswith('.zip') or x.endswith('.json')]:\n",
    "            source = os.path.join(DATA_DIR, exchange, pair, file)\n",
    "            dest = os.path.join(FLATTEN_DIR, ex+'.'+market+'.' + pair + '.' + file)\n",
    "            print(source)\n",
    "            #shutil.move(source, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exchange in os.listdir(DATA_DIR):\n",
    "    [ex,market] = exchange.split('-')\n",
    "    pairs = os.listdir(os.path.join(DATA_DIR, exchange))\n",
    "    for pair in pairs:\n",
    "        periods = os.listdir(os.path.join(DATA_DIR, exchange, pair))\n",
    "        for period in periods:\n",
    "            files = os.listdir(os.path.join(DATA_DIR, exchange, pair, period))\n",
    "            for file in [x for x in files if x.endswith('.zip') or x.endswith('.json')]:\n",
    "                source = os.path.join(DATA_DIR, exchange, pair, period, file)\n",
    "                dest = os.path.join(FLATTEN_DIR, ex+'.'+market+'.' + pair + '.' + period + '.' + file)\n",
    "                print(dest)\n",
    "                #shutil.move(source, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in [x for x in os.listdir(FLATTEN_DIR) if x.endswith('.json')]:\n",
    "    # zip -9 -rm WhaleEx.Spot.TRX_USDT.2020-06-10.zip WhaleEx.Spot.TRX_USDT.2020-06-10.json\n",
    "    filename = os.path.join(FLATTEN_DIR,file[0:-5])\n",
    "    command = f'zip -j -9 -rm {filename}.zip {filename}.json'\n",
    "    print(command)\n",
    "    p = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "    p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in [x for x in os.listdir(FLATTEN_DIR) if x.endswith('.zip')]:\n",
    "    # zip -9 -rm WhaleEx.Spot.TRX_USDT.2020-06-10.zip WhaleEx.Spot.TRX_USDT.2020-06-10.json\n",
    "    filename = os.path.join(FLATTEN_DIR,file[0:-4])\n",
    "    date_str = file.split('.')[-2]\n",
    "    command = f'unzip -j {filename}.zip -d {FLATTEN_DIR}'\n",
    "\n",
    "    p = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "    p.wait()\n",
    "    if os.path.exists(f'{os.path.join(FLATTEN_DIR, date_str)}.json'):\n",
    "        shutil.move(f'{os.path.join(FLATTEN_DIR, date_str)}.json', f'{filename}.json')\n",
    "\n",
    "    command = f'rm {filename}.zip'\n",
    "    p = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "    p.wait()\n",
    "\n",
    "    command = f'zip -j -9 -rm {filename}.zip {filename}.json'\n",
    "    print(command)\n",
    "    p = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "    p.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '/data/trade'\n",
    "OUTPUT_DIR = '/data/hashed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(input_file:str, output_dir:str)->None:\n",
    "    if input_file.endswith('.json.gz'):\n",
    "        f = gzip.open(input_file, 'rt')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    elif input_file.endswith('.zip'):\n",
    "        zf = zipfile.ZipFile(input_file, 'r')\n",
    "        assert len(zf.namelist()) == 1\n",
    "        lines = zf.read(zf.namelist()[0]).decode('UTF-8').split('\\n')\n",
    "        zf.close()\n",
    "    elif input_file.endswith('.json') or input_file.endswith('file.log'):\n",
    "        f = open(input_file, 'rt')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    else:\n",
    "        raise ValueError('Unknown file suffix ' + input_file)\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            obj = json.loads(line)\n",
    "            if obj['exchange'] == 'Bitfinex' and obj['marketType'] == 'Futures':\n",
    "                obj['marketType'] = 'Swap'  # bugfix for Bitfinex\n",
    "                line = json.dumps(obj)\n",
    "            elif obj['exchange'] == 'WhaleEx' and obj['trade_id'] != obj['raw']['tradeId']:\n",
    "                obj['trade_id'] = str(obj['raw']['tradeId'])  # bugfix for WhaleEx\n",
    "                line = json.dumps(obj)\n",
    "\n",
    "            date_str = datetime.datetime.fromtimestamp(obj['timestamp']/1000.0).isoformat()[0:10]\n",
    "            exchange = obj['exchange']\n",
    "            market_type = obj['marketType']\n",
    "            pair = obj['pair']\n",
    "            rawPair = obj['rawPair']\n",
    "            filename = f'{exchange}.{market_type}.{pair}.{rawPair}' if market_type == 'Futures' else f'{exchange}.{market_type}.{pair}'\n",
    "\n",
    "            output_file = os.path.join(output_dir, f'{filename}.{date_str}.json')\n",
    "            file_object = open(output_file, 'at')\n",
    "            file_object.write(line + '\\n')\n",
    "            file_object.close()\n",
    "    del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split('/data/trade/WhaleEx-Spot/BTC_USDT/20200511-0155-1.json.gz', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split('/home/frankdai/data/trade/WhaleEx-Spot/BTC_USDT/2020-06-06.zip', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split('/home/frankdai/data/trade/WhaleEx-Spot/BTC_USDT/2020-06-07.json', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_multi(input_dir:str, output_dir:str)->None:\n",
    "    json_files = [f for f in glob.glob(os.path.join(input_dir, \"**/*.json\"), recursive=True)]\n",
    "    zip_files = [f for f in glob.glob(os.path.join(input_dir, \"**/*.zip\"), recursive=True)]\n",
    "    gz_files = [f for f in glob.glob(os.path.join(input_dir, \"**/*.json.gz\"), recursive=True)]\n",
    "    log_files = [f for f in glob.glob(os.path.join(input_dir, \"**/file.log\"), recursive=True)]\n",
    "    files = json_files+zip_files+gz_files+log_files\n",
    "    if len(files) <= 0:\n",
    "        return\n",
    "    for file in files:\n",
    "        split(file, output_dir)\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_multi('/data/trade-20200607-20200610/', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplicate and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_and_sort(input_file:str, ouput_file:str)->None:\n",
    "    trade_map = {};\n",
    "    f = open(input_file, 'rt')\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            obj = json.loads(line)\n",
    "            exchange = obj['exchange']\n",
    "            market_type = obj['marketType']\n",
    "            pair = obj['pair']\n",
    "            raw_pair = obj['rawPair']\n",
    "            trade_id = obj['trade_id']\n",
    "            if not trade_id:  # Fix trade_id for Kraken, MXC\n",
    "                if exchange == 'Kraken' or exchange == 'MXC':\n",
    "                    obj['trade_id'] = str(obj['timestamp'])\n",
    "                    trade_id = obj['trade_id']\n",
    "                    line = json.dumps(obj)\n",
    "            if not trade_id:\n",
    "                f.close()\n",
    "                raise ValueError(line)\n",
    "            key = f'{exchange}-{market_type}-{pair}-{raw_pair}-{trade_id}'\n",
    "            trade_map[key] = line\n",
    "    f.close()\n",
    "    trade_array = []\n",
    "    for key in trade_map:\n",
    "        trade_array.append({'key': key, 'line': trade_map[key]})\n",
    "    del trade_map\n",
    "    trade_array.sort(key=lambda x: x['key'])\n",
    "\n",
    "    f = open(ouput_file, 'at')\n",
    "    for item in trade_array:\n",
    "        f.write(item['line'] + '\\n')\n",
    "    del trade_array\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_and_sort_wrapper(input_file:str, output_dir:str)->None:\n",
    "    date_str = input_file.split('.')[-2]\n",
    "    date_dir = os.path.join(output_dir, date_str)\n",
    "    if not os.path.exists(date_dir):\n",
    "        os.mkdir(date_dir)\n",
    "    dedup_and_sort(input_file, os.path.join(date_dir, os.path.basename(input_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_sort_multi(input_dir:str, output_dir:str)->None:\n",
    "    files = [f for f in glob.glob(os.path.join(input_dir, \"**/*.json\"), recursive=True)]\n",
    "    db.from_sequence(files).map(lambda file: dedup_and_sort_wrapper(file, output_dir)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_sort_multi('/data/hashed', '/data/dated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
